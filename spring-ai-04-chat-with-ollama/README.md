# ğŸŒ Spring AI + Ollama â€” AI Function Calling with Real-Time Weather Integration

## Conversational AI â€¢ Local LLM (Ollama) â€¢ Tool / Function Calling â€¢ Real-Time Weather Data

This project demonstrates how to build an **AI-powered Spring Boot application** using **Spring AI** and **Ollama** that can:

- ğŸ’¬ Answer **natural language questions about cities**
- ğŸŒ¤ï¸ **Automatically fetch real-time weather data** when relevant
- ğŸ”§ Use **Spring AI Function Calling** to connect LLMs with external APIs
- âš¡ Run efficiently with **Java 21 Virtual Threads**
- ğŸ§  Execute fully **locally** using **Ollama (LLaMA 3.x)**

The AI decides *when* to call the weather API â€” no hardcoded logic, just intelligent tool usage.

---



## ğŸš€ What This Project Showcases

- **Spring AI + Ollama integration**
- **Function calling (AI tools)** with type-safe Java records
- **External REST API integration** (WeatherAPI.com)
- **Context-aware AI responses**
- **Clean, production-ready architecture**
- **Local-first AI (no cloud dependency)**

---

## âš™ï¸ How It Works

This application connects an **AI Chat Client (Ollama)** with a **Spring AI function** that the model can invoke automatically whenever **weather information is required**.

### ğŸ”„ High-Level Execution Flow

1. ğŸŒ **Client Request**  
   The client calls the REST endpoint:  
   `GET /cities?message=...`

2. ğŸ¤– **AI Request Processing**  
   The controller forwards the userâ€™s natural language query to the configured **Spring AI `ChatClient`**.

3. ğŸ§  **Intent Analysis & Tool Selection**  
   The AI model analyzes the request.
    - If weather data would improve the response, it **automatically triggers** the registered tool:  
      `currentWeatherFunction`.

4. ğŸŒ¡ï¸ **External Weather API Call**  
   The function is implemented by **`WeatherService`**, which:
    - Calls the external Weather API endpoint: `/current.json`
    - Uses `RestClient` for HTTP communication
    - Returns a **structured, type-safe response**.

5. âœ¨ **Context-Enriched AI Response**  
   The AI receives the function result and composes a **natural, user-friendly reply** that includes **live weather data** alongside its reasoning.

> ğŸ§© This flow demonstrates **true AI tool calling**:  
> the model decides *when* and *how* to invoke external servicesâ€”no manual routing required.





## ğŸ§° Prerequisites

- â˜• **Java 21+** â€” required for **virtual threads** support
- ğŸ“¦ **Maven or Gradle** â€” for dependency management and builds
- ğŸ¦™ **Ollama Server (Local or Remote)** â€” if using Ollama as the LLM provider
    - Make sure the Ollama service is running and accessible
- ğŸŒ¤ï¸ **WeatherAPI.com API Key** (or a compatible weather provider)
- ğŸŒ **Network Access**
    - Access to the Ollama **base URL**
    - Access to the **Weather API** endpoint  

---

## Configuration

Below is an example `application.yml` fragment used by this project. Copy and paste into your application.yml or application-*.yml.

```yaml
spring:
  application:
    name: 03-chat-ollama

  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: llama3.2          # Ollama model you are using
          temperature: 0.7

  threads:
    virtual:
      enabled: true              # Enable Java 21 virtual threads for high-concurrency

logging:
  level:
    org.springframework.ai.chat.client.advisor: DEBUG

weather:
  api-key: ${WEATHER_API_KEY}   # Externalized API key from environment variable
  api-url: https://api.weatherapi.com/v1
```




