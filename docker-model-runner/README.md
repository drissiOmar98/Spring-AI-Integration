# ğŸ³ğŸ¤– Docker Model Runner with Spring AI

![Docker + Spring AI](https://img.shields.io/badge/Docker-Spring%20AI-blue)
![Java](https://img.shields.io/badge/Java-21-orange)
![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.5.x-green)
![Local AI](https://img.shields.io/badge/AI-Local%20Models-success)

## ğŸš€ Run AI Models Locally with Docker & Spring Boot

This project demonstrates how to **run AI models locally** using **Docker Desktop Model Runner** and integrate them seamlessly with **Spring AI**.

No OpenAI keys.  
No cloud dependency.  
ğŸ”’ **Your data stays local.**

Spring AI connects to Dockerâ€™s **OpenAI-compatible API**, giving you the same developer experience while keeping **full control over models, data, and infrastructure**.

---

## ğŸ§  What This Project Shows

This application showcases:

- ğŸ³ **Docker Desktop Model Runner (Beta)**
- ğŸŒ± **Spring Boot 3.5.x**
- ğŸ¤– **Spring AI abstraction layer**
- â˜• **Java 21**
- ğŸ”Œ **OpenAI-compatible local inference**

The app sends prompts to locally running models via Docker and prints the AI response using Spring AIâ€™s `ChatClient`.

---

## âœ¨ Key Features

| Feature | Description |
|------|------------|
| ğŸ§  Local AI Inference | Run LLMs locally without cloud APIs |
| ğŸ”Œ OpenAI-Compatible API | Works seamlessly with Spring AI |
| ğŸ³ Docker Model Runner | Easy model management via Docker Desktop |
| ğŸ”’ No API Keys Required | Fully offline & privacy-friendly |
| âš¡ Fast Iteration | Ideal for local development & experimentation |
| ğŸ§© Extensible | Easily swap models or add RAG, streaming, tools |

---

## ğŸ§° Requirements

Before running the project, make sure you have:

- ğŸ³ **Docker Desktop 4.40+**
    - Model Runner enabled
    - Apple Silicon (M1 / M2 / M3 recommended)
- â˜• **Java 24 JDK**
- ğŸ“¦ **Maven**
- ğŸ§  Enough **RAM** for AI models (varies by model)

---

## ğŸ“¦ Dependencies

Core dependencies used in this project:

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>

<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-openai-spring-boot-starter</artifactId>
</dependency>
```
---

## âš¡ Quickstart

### ğŸ› ï¸ 1. Enable Docker Model Runner
1. ğŸ³ Open **Docker Desktop** â†’ **Settings** â†’ **Features in development (Beta)**
2. âœ… Enable **Docker Model Runner**
3. ğŸŒ Enable **Host-side TCP support** (default port: `12434`)
4. ğŸ” Apply changes & **restart Docker Desktop**

### 2. Pull a model
Example:
```bash
docker model pull ai/gemma3
```
List models:
```bash
docker model list
```

> Tip: Use smaller models if you are low on RAM.

### 3. Configure the application
Example `application.properties`:
```properties
# No real API key needed for local Model Runner
spring.ai.openai.api-key=_
# Docker Model Runner base URL (host-side TCP)
spring.ai.openai.chat.base-url=http://localhost:12434/engines/llama.cpp
# Default model to use (must match a pulled model)
spring.ai.openai.chat.options.model=ai/gemma3
```

### 4. Build & run
Build:
```bash
./mvnw clean package
```
Run:
```bash
./mvnw spring-boot:run
```
The app will start and (if configured) run a sample `CommandLineRunner` that prompts the local model and logs the response.
